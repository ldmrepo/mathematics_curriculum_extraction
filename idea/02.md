## π¤– LLM κΈ°λ° μν•™ λ¬Έν•­ μλ™λ¶„λ¥ μ‹μ¤ν… - κ³ κΈ‰ λ¨λΈ ν™μ© μ „λµ

### 1. μƒμ© LLM API ν™μ© μ „λµ

#### 1.1 λ¨λΈλ³„ νΉμ„± λ° ν™μ© λ°©μ•

##### **OpenAI GPT-4/GPT-4-Turbo**
```
κ°•μ :
- μν•™μ  μ¶”λ΅  λ¥λ ¥ μ°μ
- Function CallingμΌλ΅ κµ¬μ΅°ν™”λ μ¶λ ¥
- Vision APIλ΅ λ„ν•/κ·Έλν”„ λ¬Έμ  μ²λ¦¬
- Assistants APIλ΅ μ§€μ†μ  μ»¨ν…μ¤νΈ κ΄€λ¦¬

ν™μ© λ°©μ•:
- λ³µμ΅ν• λ¬Έν•­ μλ―Έ λ¶„μ„ (Chain of Thought)
- μμ‹/λ„ν• ν¬ν•¨ λ¬Έν•­ μ²λ¦¬ (GPT-4V)
- λ¬Έν•­ μƒμ„± λ° λ³€ν• (Fine-tuning)
- μ±„μ  κΈ°μ¤€ λ° ν•΄μ„¤ μλ™ μƒμ„±
```

##### **Anthropic Claude 3 (Opus/Sonnet/Haiku)**
```
κ°•μ :
- κΈ΄ μ»¨ν…μ¤νΈ μ²λ¦¬ (200K tokens)
- κµμ΅μ  μ„¤λ… μƒμ„± λ¥λ ¥
- μ•μ „μ„±κ³Ό μ •ν™•μ„± μ¤‘μ‹
- Constitutional AIλ΅ νΈν–¥ μµμ†ν™”

ν™μ© λ°©μ•:
- μ „μ²΄ κµμ΅κ³Όμ • λ¬Έμ„ μ„λ² λ”©
- μƒμ„Έν• κµμ΅μ  ν”Όλ“λ°± μƒμ„±
- λ³µμ λ¬Έν•­ κ°„ κ΄€κ³„ λ¶„μ„
- κµμ΅ μ² ν•™ κΈ°λ° λ¬Έν•­ ν‰κ°€
```

##### **Google Gemini Pro/Ultra**
```
κ°•μ :
- λ©€ν‹°λ¨λ‹¬ λ„¤μ΄ν‹°λΈ μ§€μ›
- μν•™/κ³Όν•™ λ„λ©”μΈ νΉν™”
- μ‹¤μ‹κ°„ μ¶”λ΅  μ†λ„
- Vertex AI ν†µν•© MLOps

ν™μ© λ°©μ•:
- λ³µν•© λ―Έλ””μ–΄ λ¬Έν•­ μ²λ¦¬
- μ‹¤μ‹κ°„ λ¬Έν•­ λ¶„λ¥ μ„λΉ„μ¤
- λ€κ·λ¨ λ°°μΉ μ²λ¦¬
- AutoMLκ³Ό μ—°κ³„ν• μ»¤μ¤ν…€ λ¨λΈ
```

#### 1.2 ν•μ΄λΈλ¦¬λ“ μ¤μΌ€μ¤νΈλ μ΄μ…
```yaml
λ¬Έν•­ λ¶„λ¥ νμ΄ν”„λΌμΈ:
  1μ°¨ μ¤ν¬λ¦¬λ‹:
    model: Claude Haiku / GPT-3.5
    task: λΉ λ¥Έ μμ—­/ν•™λ…„ λ¶„λ¥
    cost: Low
    
  2μ°¨ μ •λ°€ λ¶„μ„:
    model: GPT-4 / Claude Opus
    task: μ„±μ·¨κΈ°μ¤€ λ§¤μΉ­, λ‚μ΄λ„ νμ •
    cost: Medium
    
  3μ°¨ κ²€μ¦/λ³΄κ°•:
    model: Gemini Pro
    task: μ‹κ° μλ£ λ¶„μ„, κµμ°¨ κ²€μ¦
    cost: Medium
    
  νΉμ μΌ€μ΄μ¤:
    model: λ„λ©”μΈ νΉν™” Fine-tuned λ¨λΈ
    task: μ—£μ§€ μΌ€μ΄μ¤, μ‹ μ ν• λ¬Έν•­
```

### 2. Fine-tuning μ „λµ

#### 2.1 OpenAI Fine-tuning μ ‘κ·Όλ²•

##### **λ°μ΄ν„° μ¤€λΉ„ μ „λµ**
```json
{
  "messages": [
    {
      "role": "system",
      "content": "λ‹Ήμ‹ μ€ ν•κµ­ μν•™ κµμ΅κ³Όμ • μ „λ¬Έκ°€μ…λ‹λ‹¤. μ£Όμ–΄μ§„ λ¬Έν•­μ„ 2022 κ°μ • κµμ΅κ³Όμ • μ„±μ·¨κΈ°μ¤€μ— λ”°λΌ λ¶„λ¥ν•μ„Έμ”."
    },
    {
      "role": "user",
      "content": "λ¬Έν•­: λ‘ μλ¦¬ μ 24μ™€ 35λ¥Ό λ”ν•μ‹μ¤."
    },
    {
      "role": "assistant",
      "content": {
        "classification": {
          "grade_level": "μ΄1-2",
          "domain": "μμ™€ μ—°μ‚°",
          "achievement_standard": "[2μ01-04]",
          "achievement_level": "B",
          "concepts": ["λ§μ…", "λ‘ μλ¦¬ μ"],
          "difficulty": 2,
          "cognitive_level": "μ΄ν•΄"
        }
      }
    }
  ]
}
```

##### **Fine-tuning κµ¬μ„±**
```python
Fine-tuning μ„¤μ •:
- Base Model: gpt-3.5-turbo-1106 or gpt-4-1106-preview
- Training Examples: 10,000+ λΌλ²¨λ§λ λ¬Έν•­
- Validation Set: 2,000 λ¬Έν•­
- Hyperparameters:
  - n_epochs: 3-5
  - batch_size: 8-16
  - learning_rate_multiplier: 0.5-2.0
  - temperature: 0.3 (μΌκ΄€μ„± μ°μ„ )
```

#### 2.2 μ¤ν”μ†μ¤ LLM Fine-tuning

##### **λ¨λΈ μ„ νƒ κΈ°μ¤€**
```
ν•κµ­μ–΄ νΉν™” λ¨λΈ:
- Polyglot-Ko (1.3B - 12.8B)
- KoAlpaca (7B, 13B)
- KULLM (7B, 13B)
- HyperCLOVA X (λΉ„κ³µκ°, APIλ§)

μν•™ νΉν™” λ¨λΈ:
- WizardMath (7B, 13B, 70B)
- MetaMath (7B, 13B, 70B)
- MAmmoTH (7B, 13B)
- Llemma (7B, 34B)

λ©€ν‹°λ§κµ¬μ–Ό λ¨λΈ:
- Llama 2/3 (7B, 13B, 70B)
- Mistral/Mixtral (7B, 8x7B)
- Qwen (7B, 14B, 72B)
- Yi (6B, 34B)
```

##### **Fine-tuning κΈ°λ²•**
```yaml
LoRA (Low-Rank Adaptation):
  μ¥μ : λ©”λ¨λ¦¬ ν¨μ¨μ , λΉ λ¥Έ ν•™μµ
  μ„¤μ •:
    r: 8-16
    lora_alpha: 16-32
    target_modules: ["q_proj", "v_proj"]
    
QLoRA (Quantized LoRA):
  μ¥μ : 4-bit μ–‘μν™”λ΅ λ©”λ¨λ¦¬ μ μ•½
  μ„¤μ •:
    bnb_4bit_compute_dtype: float16
    bnb_4bit_quant_type: "nf4"
    
Full Fine-tuning:
  μ¥μ : μµκ³  μ„±λ¥
  μ”κµ¬μ‚¬ν•­: λ†’μ€ GPU λ©”λ¨λ¦¬ (A100 80GB+)
  
Instruction Tuning:
  ν•μ‹: Alpaca, ShareGPT, OpenAI Chat
  λ°μ΄ν„°: ν•κµ­ μν•™ κµμ΅κ³Όμ • νΉν™”
```

### 3. RAG (Retrieval Augmented Generation) κµ¬ν„

#### 3.1 μ§€μ‹ λ² μ΄μ¤ κµ¬μ¶•
```
λ²΅ν„° λ°μ΄ν„°λ² μ΄μ¤ κµ¬μ„±:
β”β”€β”€ μ„±μ·¨κΈ°μ¤€ μ„λ² λ”© (181κ°)
β”‚   β”β”€β”€ μ„±μ·¨κΈ°μ¤€ μ½”λ“
β”‚   β”β”€β”€ μ„±μ·¨κΈ°μ¤€ λ‚΄μ©
β”‚   β””β”€β”€ κ΄€λ ¨ κ°λ… νƒκ·Έ
β”‚
β”β”€β”€ μ„±μ·¨μμ¤€ μ„λ² λ”© (843κ°)
β”‚   β”β”€β”€ μμ¤€λ³„ μ„¤λ…
β”‚   β””β”€β”€ ν‰κ°€ κΈ°μ¤€
β”‚
β”β”€β”€ μμ‹ λ¬Έν•­ μ„λ² λ”© (10,000+)
β”‚   β”β”€β”€ λ¬Έν•­ ν…μ¤νΈ
β”‚   β”β”€β”€ μ •λ‹µ λ¶„λ¥
β”‚   β””β”€β”€ λ©”νƒ€λ°μ΄ν„°
β”‚
β””β”€β”€ κµμ΅κ³Όμ • ν•΄μ„¤ μ„λ² λ”©
    β”β”€β”€ μμ—­λ³„ μ„¤λ…
    β”β”€β”€ ν•™λ…„λ³„ νΉμ„±
    β””β”€β”€ κµμν•™μµ λ°©λ²•
```

#### 3.2 RAG νμ΄ν”„λΌμΈ
```python
RAG μ›ν¬ν”λ΅μ°:
1. Query Embedding: μ…λ ¥ λ¬Έν•­ λ²΅ν„°ν™”
2. Similarity Search: μ μ‚¬ μ„±μ·¨κΈ°μ¤€/λ¬Έν•­ κ²€μƒ‰
3. Context Construction: κ΄€λ ¨ μ •λ³΄ μ΅°ν•©
4. Prompt Engineering: κµ¬μ΅°ν™”λ ν”„λ΅¬ν”„νΈ μƒμ„±
5. LLM Generation: μ»¨ν…μ¤νΈ κΈ°λ° λ¶„λ¥
6. Post-processing: κ²°κ³Ό κ²€μ¦ λ° μ •μ 

λ²΅ν„° DB μµμ…:
- Pinecone: κ΄€λ¦¬ν•, ν™•μ¥μ„± μ°μ
- Weaviate: ν•μ΄λΈλ¦¬λ“ κ²€μƒ‰ μ§€μ›
- Chroma: κ²½λ‰, λ΅μ»¬ κ°λ° μ ν•©
- Qdrant: μ¨ν”„λ λ―Έμ¤, κ³ μ„±λ¥
```

### 4. ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§ μ „λµ

#### 4.1 Few-shot Learning ν…ν”λ¦Ώ
```python
system_prompt = """
λ‹Ήμ‹ μ€ ν•κµ­ 2022 κ°μ • μν•™κ³Ό κµμ΅κ³Όμ • λ¶„λ¥ μ „λ¬Έκ°€μ…λ‹λ‹¤.
λ‹¤μ λ¶„λ¥ μ²΄κ³„λ¥Ό μ •ν™•ν λ”°λ¥΄μ„Έμ”:
- ν•™λ…„κµ°: [μ΄1-2, μ΄3-4, μ΄5-6, μ¤‘1-3]
- μμ—­: [μμ™€ μ—°μ‚°, λ³€ν™”μ™€ κ΄€κ³„, λ„ν•κ³Ό μΈ΅μ •, μλ£μ™€ κ°€λ¥μ„±]
- μ„±μ·¨κΈ°μ¤€: [ν•™λ…„μ½”λ“μμμ—­μ½”λ“-μλ²] ν•μ‹
- μ„±μ·¨μμ¤€: μ΄λ“±(A,B,C), μ¤‘λ“±(A,B,C,D,E)
"""

few_shot_examples = [
    {
        "input": "μ‚Όκ°ν•μ λ‚΄κ°μ ν•©μ„ κµ¬ν•μ‹μ¤.",
        "output": {
            "grade": "μ΄3-4",
            "domain": "λ„ν•κ³Ό μΈ΅μ •",
            "standard": "[4μ03-03]",
            "level": "B",
            "reasoning": "μ‚Όκ°ν•μ κΈ°λ³Έ μ„±μ§μ„ λ¬»λ” λ¬Έν•­"
        }
    },
    # ... λ” λ§μ€ μμ‹
]
```

#### 4.2 Chain of Thought (CoT) ν”„λ΅¬ν”„νΈ
```
λ¬Έν•­ λ¶„μ„ λ‹¨κ³„:
1. ν•µμ‹¬ μν•™ κ°λ… μ¶”μ¶
2. ν•„μ”ν• μ‚¬μ „ μ§€μ‹ ν™•μΈ
3. λ¬Έμ  ν•΄κ²° κ³Όμ • λ³µμ΅λ„ ν‰κ°€
4. μ μ‚¬ μ„±μ·¨κΈ°μ¤€ ν›„λ³΄ λ‚μ—΄
5. μµμΆ… μ„±μ·¨κΈ°μ¤€ μ„ νƒ λ° κ·Όκ±°
6. μ„±μ·¨μμ¤€ νμ • κ·Όκ±°
```

### 5. λ¨λΈ μ•™μƒλΈ” λ° ν‰κ°€

#### 5.1 μ•™μƒλΈ” μ „λµ
```yaml
Voting Ensemble:
  models:
    - GPT-4 (weight: 0.35)
    - Claude-3 (weight: 0.35)
    - Fine-tuned Llama (weight: 0.20)
    - Rule-based (weight: 0.10)
  
  aggregation: weighted_majority_vote
  confidence_threshold: 0.75
  
Stacking Ensemble:
  level_1:
    - OpenAI Embedding + Classifier
    - Claude RAG Pipeline
    - Fine-tuned Model
  level_2:
    meta_learner: XGBoost
```

#### 5.2 ν‰κ°€ λ©”νΈλ¦­
```python
ν‰κ°€ μ§€ν‘:
- Accuracy: μ „μ²΄ μ •ν™•λ„
- F1-Score: ν΄λμ¤λ³„ κ· ν• ν‰κ°€
- Cohen's Kappa: ν‰κ°€μ κ°„ μΌμΉλ„
- Mean Reciprocal Rank: μμ„ μμΈ΅ μ •ν™•λ„
- Confidence Calibration: μ‹ λΆ°λ„ λ³΄μ •

A/B ν…μ¤νΈ:
- Champion Model: ν„μ¬ ν”„λ΅λ•μ… λ¨λΈ
- Challenger Model: μƒλ΅μ΄ λ¨λΈ/λ²„μ „
- Test Duration: 2μ£Ό
- Success Metrics: +5% accuracy improvement
```

### 6. λΉ„μ© μµμ ν™” μ „λµ

#### 6.1 κ³„μΈµμ  λΌμ°ν…
```
νΈλν”½ λΌμ°ν…:
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  μ…λ ¥ λ¬Έν•­   β”‚
β””β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”
       β†“
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚ λ‚μ΄λ„ νλ³„κΈ° β”‚ β†’ Simple: GPT-3.5/Claude Haiku (80%)
β””β”€β”€β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”€β”€β”€β”
       β†“
    Complex: GPT-4/Claude Opus (20%)
       β†“
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  Edge Cases  β”‚ β†’ Fine-tuned Model (5%)
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
```

#### 6.2 μΊμ‹± μ „λµ
```python
μΊμ‹± λ λ²¨:
1. Exact Match Cache: λ™μΌ λ¬Έν•­ μ¦‰μ‹ λ°ν™
2. Semantic Cache: μ μ‚¬ λ¬Έν•­ κ²°κ³Ό μ°Έμ΅°
3. Partial Cache: μ¤‘κ°„ κ²°κ³Ό μ¬μ‚¬μ©
4. Model Cache: μ„λ² λ”© λ²΅ν„° μ €μ¥

TTL μ„¤μ •:
- Hot Cache: 1μ‹κ°„ (μμ£Ό μ΅°ν)
- Warm Cache: 24μ‹κ°„ (μΌλ°)
- Cold Cache: 7μΌ (λ“λ¬Όκ² μ΅°ν)
```

### 7. ν”„λ΅λ•μ… κ³ λ ¤μ‚¬ν•­

#### 7.1 MLOps νμ΄ν”„λΌμΈ
```yaml
CI/CD Pipeline:
  develop:
    - λ°μ΄ν„° κ²€μ¦
    - λ¨λΈ ν•™μµ
    - λ‹¨μ„ ν…μ¤νΈ
    
  staging:
    - A/B ν…μ¤νΈ
    - μ„±λ¥ λ²¤μΉλ§ν¬
    - λ¶€ν• ν…μ¤νΈ
    
  production:
    - Blue-Green λ°°ν¬
    - λ¨λ‹ν„°λ§
    - λ΅¤λ°± μ¤€λΉ„
```

#### 7.2 λ¨λ‹ν„°λ§ λ° κ΄€μ°°μ„±
```
λ¨λ‹ν„°λ§ λ€μ‹λ³΄λ“:
- API μ‘λ‹µ μ‹κ°„ (P50, P95, P99)
- λ¨λΈ μ •ν™•λ„ νΈλ λ“
- λΉ„μ© μ¶”μ  (λ¨λΈλ³„, μ‹κ°„λ€λ³„)
- μ—λ¬μ¨ λ° μ¬μ‹λ„μ¨
- μ‚¬μ©μ ν”Όλ“λ°± μ¤μ½”μ–΄

μ•λ¦Ό μ„¤μ •:
- μ •ν™•λ„ ν•λ½ > 5%
- μ‘λ‹µ μ‹κ°„ > 2μ΄
- λΉ„μ© κΈ‰μ¦ > 150%
- μ—λ¬μ¨ > 1%
```

μ΄λ¬ν• κ³ κΈ‰ λ¨λΈ ν™μ© μ „λµμ„ ν†µν•΄ λ†’μ€ μ •ν™•λ„μ™€ ν¨μ¨μ„±μ„ κ°–μ¶ λ¬Έν•­ λ¶„λ¥ μ‹μ¤ν…μ„ κµ¬μ¶•ν•  μ μμΌλ©°, μ§€μ†μ μΈ κ°μ„ κ³Ό ν™•μ¥μ΄ κ°€λ¥ν• κµ¬μ΅°λ¥Ό λ§λ“¤ μ μμµλ‹λ‹¤.