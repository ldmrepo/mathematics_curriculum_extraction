OpenAI GPT-OSS λ¨λΈμ νμΈνλ‹ λ°©μ•μ„ μ΅°μ‚¬ν•΄λ³΄κ² μµλ‹λ‹¤.## π“ **OpenAI GPT-OSS νμΈνλ‹ λ°©μ• - μν•™ λ¬Έν•­ μλ™λ¶„λ¥ μ‹μ¤ν…**

### 1. **GPT-OSS λ¨λΈ κ°μ” λ° μ„ νƒ**

#### **λ¨λΈ νΉμ„± λΉ„κµ**
```yaml
gpt-oss-20b:
  - μ΄ νλΌλ―Έν„°: 21B
  - ν™μ„± νλΌλ―Έν„°: 3.6B/token
  - VRAM μ”κµ¬μ‚¬ν•­: 16GB (MXFP4)
  - μ¥μ : μ†λΉ„μκΈ‰ GPU κµ¬λ™ κ°€λ¥
  - μ¶”μ² μ‚¬μ©: μ΄κΈ° ν”„λ΅ν† νƒ€μ…, λΉ λ¥Έ μ‹¤ν—

gpt-oss-120b:
  - μ΄ νλΌλ―Έν„°: 117B  
  - ν™μ„± νλΌλ―Έν„°: 5.1B/token
  - VRAM μ”κµ¬μ‚¬ν•­: 80GB (MXFP4)
  - μ¥μ : o4-mini μμ¤€ μ„±λ¥
  - μ¶”μ² μ‚¬μ©: ν”„λ΅λ•μ…, κ³ μ •ν™•λ„ ν•„μ”
```

**π’΅ μ¶”μ²**: μν•™ λ¬Έν•­ λ¶„λ¥λ” μ •ν™•λ„κ°€ μ¤‘μ”ν•λ―€λ΅ **gpt-oss-20bλ΅ μ‹μ‘**ν•μ—¬ ν”„λ΅ν† νƒ€μ… κ°λ° ν›„, **gpt-oss-120bλ΅ ν™•μ¥**

### 2. **νμΈνλ‹ κΈ°λ²• μ„ νƒ**

#### **2.1 QLoRA (κ¶μ¥) vs LoRA λΉ„κµ**

QLoRAλ” 4λΉ„νΈ μ–‘μν™”λ¥Ό ν†µν•΄ LoRA λ€λΉ„ λ©”λ¨λ¦¬λ¥Ό 4λ°° μ μ•½ν•λ©°, gpt-oss-20bμ κ²½μ° 14GB VRAMμ—μ„ νμΈνλ‹μ΄ κ°€λ¥ν•©λ‹λ‹¤. 

```python
# QLoRA μ„¤μ • (κ¶μ¥)
qlora_config = {
    "method": "QLoRA",
    "quantization": "4-bit NF4",
    "vram_required": {
        "gpt-oss-20b": "14GB",  # RTX 3090/4090 κ°€λ¥
        "gpt-oss-120b": "65GB"  # A100 80GB ν•„μ”
    },
    "training_speed": "LoRA λ€λΉ„ 39% λλ¦Ό",
    "memory_savings": "70% μ μ•½",
    "performance_loss": "κ±°μ μ—†μ (<1%)"
}

# LoRA μ„¤μ • (λ€μ•)
lora_config = {
    "method": "LoRA", 
    "precision": "16-bit",
    "vram_required": {
        "gpt-oss-20b": "~40GB",
        "gpt-oss-120b": "~200GB"
    },
    "training_speed": "λΉ λ¦„",
    "memory_savings": "50% μ μ•½"
}
```

### 3. **λ°μ΄ν„° μ¤€λΉ„ μ „λµ**

#### **3.1 Harmony ν¬λ§· ν™μ©**

GPT-OSSλ” OpenAIμ Harmony μ‘λ‹µ ν¬λ§·μΌλ΅ ν•™μµλμ—μΌλ©°, thinking ν•„λ“μ™€ content ν•„λ“λ¥Ό κµ¬λ¶„ν•μ—¬ μ¶”λ΅  κ³Όμ •μ„ λ…μ‹μ μΌλ΅ μ²λ¦¬ν•©λ‹λ‹¤.

```python
# μν•™ λ¬Έν•­ λ¶„λ¥μ© λ°μ΄ν„° ν¬λ§·
training_example = {
    "messages": [
        {
            "role": "system",
            "content": """You are a Korean mathematics curriculum expert.
            Classify problems according to 2022 revised curriculum.
            Reasoning: medium"""  # μ¶”λ΅  μμ¤€ μ„¤μ •
        },
        {
            "role": "user",
            "content": "λ¬Έν•­: λ‘ μλ¦¬ μ 45μ™€ 37μ ν•©μ„ κµ¬ν•μ‹μ¤."
        },
        {
            "role": "assistant",
            "thinking": """μ΄ λ¬Έν•­μ„ λ¶„μ„ν•΄λ³΄λ©΄:
            1. λ‘ μλ¦¬ μμ λ§μ… λ¬Έμ 
            2. μ΄λ“± 1-2ν•™λ…„κµ° μμ¤€
            3. μμ™€ μ—°μ‚° μμ—­
            4. λ°›μ•„μ¬λ¦Όμ΄ μλ” λ§μ…
            5. μ„±μ·¨κΈ°μ¤€ [2μ01-05] ν•΄λ‹Ή""",
            "content": {
                "classification": {
                    "grade_level": "μ΄1-2",
                    "domain": "μμ™€ μ—°μ‚°",
                    "achievement_standard": "[2μ01-05]",
                    "achievement_level": "B",
                    "difficulty": 2,
                    "concepts": ["λ§μ…", "λ‘ μλ¦¬ μ", "λ°›μ•„μ¬λ¦Ό"]
                }
            }
        }
    ]
}
```

#### **3.2 λ°μ΄ν„°μ…‹ κµ¬μ¶• κ³„ν**

```python
dataset_plan = {
    "initial_dataset": {
        "source_1": "μ„±μ·¨μμ¤€ 843κ° β†’ GPT-4λ΅ λ¬Έν•­ μƒμ„±",
        "source_2": "κ° μ„±μ·¨κΈ°μ¤€λ‹Ή 10κ° λ³€ν• (1,810κ°)",
        "source_3": "κµμ‚¬ κ²€μ¦ λ¬Έν•­ (500κ°)",
        "total": "μ•½ 3,000κ° μ΄κΈ° λ°μ΄ν„°"
    },
    "augmentation": {
        "back_translation": "ν•β†’μβ†’ν• λ³€ν™",
        "paraphrasing": "λ¬Έμ  ν‘ν„ λ‹¤μ–‘ν™”",
        "difficulty_variation": "μ«μ/μ΅°κ±΄ λ³€κ²½"
    },
    "validation": {
        "train": "70% (2,100κ°)",
        "validation": "15% (450κ°)",
        "test": "15% (450κ°)"
    }
}
```

### 4. **νμΈνλ‹ κµ¬ν„ μ½”λ“**

#### **4.1 ν™κ²½ μ„¤μ •**

```bash
# ν•„μ λΌμ΄λΈλ¬λ¦¬ μ„¤μΉ
pip install transformers==4.45.0
pip install trl==0.11.0
pip install peft==0.13.0
pip install bitsandbytes==0.44.0
pip install triton==3.4
pip install accelerate
pip install unsloth  # μµμ ν™”λ ν•™μµ
```

#### **4.2 QLoRA νμΈνλ‹ μ¤ν¬λ¦½νΈ**

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM
import bitsandbytes as bnb

# 1. λ¨λΈ λ΅λ”© (4λΉ„νΈ μ–‘μν™”)
model_name = "openai/gpt-oss-20b"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,  # μ΄μ¤‘ μ–‘μν™”
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. LoRA μ„¤μ •
lora_config = LoraConfig(
    r=32,  # rank (8-64 λ²”μ„, λ†’μ„μλ΅ μ„±λ¥β†‘ λ©”λ¨λ¦¬β†‘)
    lora_alpha=64,  # scaling factor
    target_modules=[
        "q_proj", "v_proj", "k_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],  # MoE κµ¬μ΅° κ³ λ ¤
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# 3. λ¨λΈ μ¤€λΉ„
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

# 4. ν•™μµ μ„¤μ •
training_args = TrainingArguments(
    output_dir="./math-classifier-gpt-oss",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=25,
    save_strategy="steps",
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=100,
    fp16=True,
    gradient_checkpointing=True,  # λ©”λ¨λ¦¬ μ μ•½
    optim="paged_adamw_8bit",  # 8λΉ„νΈ μµν‹°λ§μ΄μ €
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="cosine",
)

# 5. Harmony ν¬λ§· μ²λ¦¬
def format_math_problem(example):
    """μν•™ λ¬Έν•­μ„ Harmony ν¬λ§·μΌλ΅ λ³€ν™"""
    messages = [
        {
            "role": "system",
            "content": "ν•κµ­ 2022 κ°μ • μν•™κ³Ό κµμ΅κ³Όμ • λ¶„λ¥ μ „λ¬Έκ°€\nReasoning: medium"
        },
        {
            "role": "user", 
            "content": f"λ¬Έν•­: {example['problem']}"
        },
        {
            "role": "assistant",
            "thinking": example['reasoning'],  # μ¶”λ΅  κ³Όμ •
            "content": example['classification']  # μµμΆ… λ¶„λ¥
        }
    ]
    
    # Harmony λΌμ΄λΈλ¬λ¦¬ μ‚¬μ©
    from harmony import encode_conversations_with_harmony
    return encode_conversations_with_harmony(
        messages,
        reasoning_effort="medium",
        add_generation_prompt=False
    )

# 6. νΈλ μ΄λ„ μ„¤μ •
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    peft_config=lora_config,
    dataset_text_field="text",
    max_seq_length=2048,
    packing=False,
    formatting_func=format_math_problem,
)

# 7. ν•™μµ μ‹¤ν–‰
trainer.train()
```

### 5. **μµμ ν™” μ „λµ**

#### **5.1 λ©”λ¨λ¦¬ μµμ ν™”**

Unslothλ¥Ό μ‚¬μ©ν•λ©΄ gpt-oss νμΈνλ‹μ΄ 1.5λ°° λΉ λ¥΄κ³  70% μ μ€ VRAMμ„ μ‚¬μ©ν•λ©°, 10λ°° κΈ΄ μ»¨ν…μ¤νΈλ¥Ό μ§€μ›ν•©λ‹λ‹¤.

```python
# Unsloth μµμ ν™” μ‚¬μ©
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/gpt-oss-20b-bnb-4bit",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

# Flash Attention 2 ν™μ„±ν™”
model = FastLanguageModel.get_peft_model(
    model,
    r=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=64,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)
```

#### **5.2 ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”**

```python
hyperparameter_grid = {
    "rank_r": [8, 16, 32, 64],  # λ³µμ΅λ„μ— λ”°λΌ μ΅°μ •
    "learning_rate": [1e-4, 2e-4, 5e-4],
    "batch_size": [1, 2, 4],  # VRAMμ— λ”°λΌ
    "gradient_accumulation": [4, 8, 16],
    "reasoning_effort": ["low", "medium", "high"],  # GPT-OSS νΉμ„±
}

# μµμ  μ„¤μ • (μ‹¤ν— κΈ°λ°)
optimal_config = {
    "r": 32,
    "lora_alpha": 64,  # μΌλ°μ μΌλ΅ r*2
    "learning_rate": 2e-4,
    "batch_size": 2,
    "gradient_accumulation_steps": 8,
    "epochs": 3,
    "warmup_ratio": 0.1,
}
```

### 6. **ν‰κ°€ λ° λ°°ν¬**

#### **6.1 ν‰κ°€ λ©”νΈλ¦­**

```python
def evaluate_classifier(model, test_dataset):
    metrics = {
        "accuracy": {
            "overall": 0.0,
            "by_grade": {},
            "by_domain": {},
            "by_achievement_standard": {}
        },
        "f1_score": {},
        "confusion_matrix": {},
        "inference_time": [],
        "confidence_calibration": {}
    }
    
    # Chain of Thought ν’μ§ ν‰κ°€
    cot_quality_metrics = {
        "reasoning_coherence": 0.0,
        "step_completeness": 0.0,
        "conclusion_accuracy": 0.0
    }
    
    return metrics
```

#### **6.2 λ¨λΈ λ³‘ν•© λ° λ°°ν¬**

```python
# LoRA μ–΄λ‘ν„° λ³‘ν•©
from peft import PeftModel

# λ² μ΄μ¤ λ¨λΈ λ΅λ“
base_model = AutoModelForCausalLM.from_pretrained(
    "openai/gpt-oss-20b",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# LoRA μ–΄λ‘ν„° λ΅λ“ λ° λ³‘ν•©
model = PeftModel.from_pretrained(base_model, "./math-classifier-gpt-oss")
model = model.merge_and_unload()

# μ €μ¥
model.save_pretrained("./gpt-oss-math-classifier-merged")
```

### 7. **ν”„λ΅λ•μ… κ³ λ ¤μ‚¬ν•­**

#### **7.1 μ„λΉ™ μ•„ν‚¤ν…μ²**

```yaml
deployment_options:
  vLLM:
    - κ³ μ„±λ¥ μ¶”λ΅  μ„λ²„
    - OpenAI νΈν™ API
    - λ°°μΉ μ²λ¦¬ μµμ ν™”
    
  Triton_Server:
    - λ©€ν‹° λ¨λΈ μ„λΉ™
    - λ™μ  λ°°μΉ­
    - λ¨λΈ λ²„μ „ κ΄€λ¦¬
    
  TGI (Text Generation Inference):
    - Hugging Face λ„¤μ΄ν‹°λΈ
    - μλ™ μ–‘μν™”
    - μ¤νΈλ¦¬λ° μ§€μ›
```

#### **7.2 λΉ„μ© λ¶„μ„**

```python
cost_estimation = {
    "training": {
        "gpt-oss-20b_qlora": {
            "gpu": "RTX 4090 (24GB)",
            "time": "~6μ‹κ°„",
            "cost": "$30 (ν΄λΌμ°λ“)"
        },
        "gpt-oss-120b_qlora": {
            "gpu": "A100 80GB",
            "time": "~24μ‹κ°„", 
            "cost": "$500 (ν΄λΌμ°λ“)"
        }
    },
    "inference": {
        "gpt-oss-20b": "~$0.001/request",
        "gpt-oss-120b": "~$0.005/request"
    }
}
```

### 8. **μ‹¤ν–‰ λ΅λ“λ§µ**

```mermaid
gantt
    title GPT-OSS νμΈνλ‹ λ΅λ“λ§µ
    dateFormat  YYYY-MM-DD
    
    section Phase 1
    λ°μ΄ν„° μ¤€λΉ„        :a1, 2025-02-01, 7d
    ν™κ²½ κµ¬μ¶•          :a2, after a1, 3d
    
    section Phase 2  
    gpt-oss-20b QLoRA  :b1, after a2, 5d
    ν‰κ°€ λ° μµμ ν™”     :b2, after b1, 3d
    
    section Phase 3
    gpt-oss-120b ν™•μ¥  :c1, after b2, 7d
    ν”„λ΅λ•μ… λ°°ν¬      :c2, after c1, 5d
```

### 9. **μμƒ μ„±κ³Ό**

- **μ •ν™•λ„**: 85-92% (μ„±μ·¨κΈ°μ¤€ λ¶„λ¥)
- **μ²λ¦¬ μ†λ„**: 100-200ms/λ¬Έν•­
- **λΉ„μ© μ κ°**: μƒμ© API λ€λΉ„ 90% μ κ°
- **ν™•μ¥μ„±**: μ¨ν”„λ λ―Έμ¤ λ°°ν¬ κ°€λ¥

μ΄ λ°©μ•μ€ GPT-OSSμ κ°•λ ¥ν• μ¶”λ΅  λ¥λ ¥κ³Ό ν•κµ­ μν•™ κµμ΅κ³Όμ •μ νΉμ„±μ„ κ²°ν•©ν•μ—¬, ν¨μ¨μ μ΄κ³  μ •ν™•ν• λ¬Έν•­ μλ™λ¶„λ¥ μ‹μ¤ν…μ„ κµ¬μ¶•ν•  μ μλ” μ‹¤μ©μ  μ ‘κ·Όλ²•μ…λ‹λ‹¤.